#MAIN
import sys
import os
import cv2
import numpy as np
import torch
import onnxruntime
from context_module.temporal_memory import TemporalLSTM
from context_module.buffer import SequenceBuffer

cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print(" Failed to access camera")
    exit()

device = 'cuda' if torch.cuda.is_available() else 'cpu'

session_light = onnxruntime.InferenceSession("trafficLight_detector.onnx", providers=["CPUExecutionProvider"])
session_sign = onnxruntime.InferenceSession("trafficSign_detector.onnx", providers=["CPUExecutionProvider"])
session_bulb = onnxruntime.InferenceSession("efficientnet_bulb_classifier.onnx", providers=["CPUExecutionProvider"])
session_sign_cls = onnxruntime.InferenceSession("efficientnetb0_gtsrb.onnx", providers=["CPUExecutionProvider"])

temporal_model = TemporalLSTM().to(device).eval()
buffer = SequenceBuffer(max_len=5, num_classes=7)

light_classes = ['stop', 'go', 'warning', 'stopLeft', 'goLeft', 'warningLeft', 'goForward']
sign_classes = [
    "speed_limit_20", "speed_limit_30", "speed_limit_50", "speed_limit_60", "speed_limit_70",
    "speed_limit_80", "end_speed_limit_80", "speed_limit_100", "speed_limit_120",
    "no_overtaking", "no_overtaking_trucks", "priority_road", "yield", "stop", "no_entry",
    "no_motor_vehicles", "no_trucks", "no_entry_any", "general_caution", "curve_left", "curve_right",
    "double_curve", "bumpy_road", "slippery_road", "road_narrows", "construction", "traffic_signal",
    "pedestrian_crossing", "children_crossing", "bikes_crossing", "snow", "animals_crossing",
    "restriction_ends", "turn_right", "turn_left", "straight_only", "straight_or_right",
    "straight_or_left", "keep_right", "keep_left", "roundabout", "restriction_ends_overtaking",
    "restriction_ends_trucks"
]


def preprocess(img, size):
    resized = cv2.resize(img, (size, size))
    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
    tensor = rgb.transpose(2, 0, 1).astype(np.float32) / 255.0
    return np.expand_dims(tensor, axis=0), resized

def postprocess(pred, size, conf_thres=0.98, min_box_size=30): 
    preds = pred[0]
    if preds.ndim == 3:
        preds = preds[0]
    boxes = []
    for i in range(preds.shape[0]):
        det = preds[i]
        if isinstance(det, np.ndarray) or isinstance(det, list):
            det = np.array(det).flatten()
        if float(det[4]) < conf_thres:
            continue
        cx, cy, w, h = det[:4]
        x1 = int((cx - w / 2) * size)
        y1 = int((cy - h / 2) * size)
        x2 = int((cx + w / 2) * size)
        y2 = int((cy + h / 2) * size)
        if (x2 - x1) < min_box_size or (y2 - y1) < min_box_size:
            continue
        boxes.append((x1, y1, x2, y2, float(det[4]), int(det[5])))
    return boxes


def classify(image, session, input_size):
    resized = cv2.resize(image, (input_size, input_size))
    tensor = resized.astype(np.float32) / 255.0
    tensor = np.transpose(tensor, (2, 0, 1))[None]
    output = session.run(None, {session.get_inputs()[0].name: tensor})[0]
    return int(np.argmax(output))

frame_id = 0
while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame_id += 1
    if frame_id % 2 != 0:
        continue

    img_light, _ = preprocess(frame, 320)
    img_sign, _ = preprocess(frame, 640)

    out_light = session_light.run(None, {session_light.get_inputs()[0].name: img_light})
    out_sign = session_sign.run(None, {session_sign.get_inputs()[0].name: img_sign})

    det_light = postprocess(out_light, 320)
    det_sign = postprocess(out_sign, 640)

    light_detected = False
    for x1, y1, x2, y2, conf, _ in det_light:
        crop = frame[y1:y2, x1:x2]
        if crop.size == 0:
            continue
        pred = classify(crop, session_bulb, 224)
        buffer.add(pred)
        label = light_classes[pred]
        light_detected = True
        print(f"Traffic Light Detected: {label}")
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    if light_detected:
        seq = buffer.get_sequence()
        if seq is not None:
            t_in = torch.tensor(seq).float().to(device)
            with torch.no_grad():
                t_out = temporal_model(t_in)
                t_pred = torch.argmax(t_out, dim=1).item()
                smoothed = light_classes[t_pred]
                cv2.putText(frame, f"Smoothed: {smoothed}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 255), 2)

    for x1, y1, x2, y2, conf, _ in det_sign:
        crop = frame[y1:y2, x1:x2]
        if crop.size == 0:
            continue
        pred = classify(crop, session_sign_cls, 224)
        label = sign_classes[pred]
        print(f"Traffic Sign Detected: Class {label}")
        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
        cv2.putText(frame, f"Sign {label}", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

    cv2.imshow("Raspberry Pi Traffic Detection", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
